--------------------------------------------------------
import { createClient } from '@/utils/supabase/server' 
import { NextResponse } from 'next/server'

export async function POST(request: Request) {
  const supabase = await createClient(); // âœ… Use await here

  const { data: { user }, error: authError } = await supabase.auth.getUser();

  if (authError || !user) {
    return NextResponse.json({ error: 'No autenticado' }, { status: 401 });
  }

  const { message } = await request.json();

  try {
    // 1. Guardar mensaje del usuario
    const { error: userMessageError } = await supabase
      .from('chats')
      .insert([{
        user_id: user.id,
        content: message,
        role: 'user'
      }]);

    if (userMessageError) throw userMessageError;

    // 2. Obtener historial de chat
    const { data: history } = await supabase
      .from('chats')
      .select('content, role')
      .order('created_at', { ascending: true })
      .limit(10);

    // 3. Llamar a LM Studio
    const lmResponse = await fetch('http://192.168.0.13:1234/v1/chat/completions', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        messages: [
          ...(history?.map((msg: { role: string; content: string }) => ({
            role: msg.role,
            content: msg.content
          })) || []),
          { role: 'user', content: message }
        ],
        temperature: 0.7,
        max_tokens: -1,
        stream: false
      })
    });

    const responseData = await lmResponse.json();
    const assistantContent = responseData.choices[0].message.content;

    // 4. Guardar respuesta del asistente
    const { error: assistantMessageError } = await supabase
      .from('chats')
      .insert([{
        user_id: user.id,
        content: assistantContent,
        role: 'assistant'
      }]);

    if (assistantMessageError) throw assistantMessageError;

    return NextResponse.json({ content: assistantContent });
    
  } catch (error) {
    console.error('Error:', error);
    return NextResponse.json(
      { error: 'Error procesando el mensaje' },
      { status: 500 }
    );
  }
}

-----------------------------------------------------------
import { createClient } from "@/utils/supabase/server";
import { NextResponse } from "next/server";

export async function POST(request: Request) {
  const supabase = await createClient();
  const { data: { user }, error: authError } = await supabase.auth.getUser();

  if (authError || !user) {
    return NextResponse.json({ error: "No autenticado" }, { status: 401 });
  }

  const { message } = await request.json();

  try {
    // ðŸ”¹ 1. Guardar mensaje del usuario en la BD
    await supabase.from("chats").insert([{ user_id: user.id, content: message, role: "user" }]);

    // ðŸ”¹ 2. Obtener historial de chat
    const { data: history } = await supabase
      .from("chats")
      .select("content, role")
      .order("created_at", { ascending: true })
      .limit(20);

    // ðŸ”¹ 3. Enviar mensaje a LM Studio (Streaming)
    const lmResponse = await fetch("http://192.168.0.13:1234/v1/chat/completions", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        messages: [
          ...(history?.map((msg: { role: string; content: string }) => ({
            role: msg.role,
            content: msg.content,
          })) || []),
          { role: "user", content: message },
        ],
        // ðŸ”¥ Control de creatividad (0.6 = mÃ¡s coherente, menos caÃ³tico)
        temperature: 0.6,
        top_p: 0.9,
        max_tokens: 1024,
        stream: true, 
      }),
    });

    if (!lmResponse.ok) throw new Error(`Error en la API de LM Studio: ${lmResponse.statusText}`);

    const reader = lmResponse.body?.getReader();
    if (!reader) throw new Error("No se pudo leer la respuesta del modelo.");

    const decoder = new TextDecoder();
    let assistantContent = "";

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      const chunk = decoder.decode(value, { stream: true });

      // ðŸ”¹ Dividir en lÃ­neas y procesar cada `data: {}` correctamente
      const lines = chunk.split("\n").map((line) => line.trim()).filter((line) => line.startsWith("data:"));

      for (const line of lines) {
        try {
          const jsonStr = line.replace(/^data:\s*/, ""); // âœ… Eliminar "data: "
          const parsedChunk = JSON.parse(jsonStr);

          if (parsedChunk.choices && parsedChunk.choices[0].delta.content) {
            assistantContent += parsedChunk.choices[0].delta.content;
          }

          if (parsedChunk.choices && parsedChunk.choices[0].finish_reason === "stop") {
            break; // âœ… Detener cuando el modelo termine su respuesta
          }
        } catch (jsonError) {
          console.warn("Error parseando JSON chunk:", jsonError);
        }
      }
    }

    // ðŸ”¹ 4. Guardar respuesta del modelo en la BD
    await supabase.from("chats").insert([{ user_id: user.id, content: assistantContent, role: "assistant" }]);

    return NextResponse.json({ content: assistantContent });
  } catch (error) {
    console.error("Error:", error);
    return NextResponse.json({ error: "Error procesando el mensaje" }, { status: 500 });
  }
}
-----------------------------------------------------------------------------------------------------------------

import { createClient } from "@/utils/supabase/server";
import { NextResponse } from "next/server";

export async function POST(request: Request) {
  const supabase = await createClient();
  const { data: { user }, error: authError } = await supabase.auth.getUser();

  if (authError || !user) {
    return NextResponse.json({ error: "No autenticado" }, { status: 401 });
  }

  const { message } = await request.json();

  try {
    // ðŸ”¹ 1. Guardar mensaje del usuario en la BD
    await supabase.from("chats").insert([{ user_id: user.id, content: message, role: "user" }]);

    // ðŸ”¹ 2. Obtener historial de chat
    const { data: history } = await supabase
      .from("chats")
      .select("content, role")
      .order("created_at", { ascending: true })
      .limit(20);

    // ðŸ”¹ 3. Enviar mensaje a LM Studio con instrucciones mÃ¡s precisas
    const lmResponse = await fetch("http://192.168.0.13:1234/v1/chat/completions", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        messages: [
          {
            role: "system",
            content: "Eres un asistente Ãºtil. Responde exclusivamente en el mismo idioma en el que se te hizo la pregunta. No muestres razonamientos previos como '<think>'.",
          },
          ...(history?.map((msg: { role: string; content: string }) => ({
            role: msg.role,
            content: msg.content,
          })) || []),
          { role: "user", content: message },
        ],
        temperature: 0.6,
        top_p: 0.9,
        max_tokens: 1024,
        stop: ["<think>"], // ðŸš€ Evita que se generen razonamientos
        stream: true, // âœ… Streaming activado
      }),
    });

    if (!lmResponse.ok) throw new Error(`Error en la API de LM Studio: ${lmResponse.statusText}`);

    const reader = lmResponse.body?.getReader();
    if (!reader) throw new Error("No se pudo leer la respuesta del modelo.");

    const decoder = new TextDecoder();
    let assistantContent = "";

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      const chunk = decoder.decode(value, { stream: true });

      // ðŸ”¹ Dividir en lÃ­neas y procesar cada `data: {}` correctamente
      const lines = chunk.split("\n").map((line) => line.trim()).filter((line) => line.startsWith("data:"));

      for (const line of lines) {
        try {
          const jsonStr = line.replace(/^data:\s*/, ""); // âœ… Eliminar "data: "
          const parsedChunk = JSON.parse(jsonStr);

          if (parsedChunk.choices && parsedChunk.choices[0].delta.content) {
            assistantContent += parsedChunk.choices[0].delta.content;
          }

          if (parsedChunk.choices && parsedChunk.choices[0].finish_reason === "stop") {
            break; // âœ… Detener cuando el modelo termine su respuesta
          }
        } catch (jsonError) {
          console.warn("Error parseando JSON chunk:", jsonError);
        }
      }
    }

    // ðŸ”¹ 4. Guardar respuesta del modelo en la BD
    await supabase.from("chats").insert([{ user_id: user.id, content: assistantContent, role: "assistant" }]);

    return NextResponse.json({ content: assistantContent });
  } catch (error) {
    console.error("Error:", error);
    return NextResponse.json({ error: "Error procesando el mensaje" }, { status: 500 });
  }
}

